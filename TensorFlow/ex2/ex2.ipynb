{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "example project 2 for tensorflow\n",
    "----\n",
    "\n",
    "\n",
    "This project is for learning tensorflow by section 2, in *this* example, we will learn about the basic linear regression model.\n",
    "\n",
    "It contains:\n",
    "* operation\n",
    "* gradient descent\n",
    "* training(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.30000001  0.60000002  0.90000004]\n",
      "4.01814\n",
      "1.81987\n",
      "1.54482\n",
      "1.48251\n",
      "1.4444\n",
      "1.4097\n",
      "1.3761\n",
      "1.34334\n",
      "1.31135\n",
      "1.28013\n",
      "1.24966\n",
      "1.21991\n",
      "1.19086\n",
      "1.16251\n",
      "1.13483\n",
      "1.10782\n",
      "1.08144\n",
      "1.05569\n",
      "1.03056\n",
      "1.00603\n",
      "0.982074\n",
      "0.958693\n",
      "0.935869\n",
      "0.913588\n",
      "0.891837\n",
      "0.870604\n",
      "0.849877\n",
      "0.829644\n",
      "0.809892\n",
      "0.79061\n",
      "0.771787\n",
      "0.753412\n",
      "0.735476\n",
      "0.717965\n",
      "0.700872\n",
      "0.684186\n",
      "0.667897\n",
      "0.651996\n",
      "0.636473\n",
      "0.62132\n",
      "0.606528\n",
      "0.592088\n",
      "0.577992\n",
      "0.564231\n",
      "0.550798\n",
      "0.537684\n",
      "0.524883\n",
      "0.512387\n",
      "0.500188\n",
      "0.48828\n",
      "0.476655\n",
      "0.465307\n",
      "0.454229\n",
      "0.443415\n",
      "0.432858\n",
      "0.422553\n",
      "0.412493\n",
      "0.402672\n",
      "0.393085\n",
      "0.383727\n",
      "0.374591\n",
      "0.365673\n",
      "0.356967\n",
      "0.348468\n",
      "0.340172\n",
      "0.332073\n",
      "0.324167\n",
      "0.31645\n",
      "0.308916\n",
      "0.301561\n",
      "0.294382\n",
      "0.287373\n",
      "0.280531\n",
      "0.273853\n",
      "0.267333\n",
      "0.260968\n",
      "0.254755\n",
      "0.24869\n",
      "0.242769\n",
      "0.236989\n",
      "0.231347\n",
      "0.225839\n",
      "0.220463\n",
      "0.215214\n",
      "0.21009\n",
      "0.205088\n",
      "0.200206\n",
      "0.195439\n",
      "0.190786\n",
      "0.186244\n",
      "0.18181\n",
      "0.177481\n",
      "0.173256\n",
      "0.169131\n",
      "0.165104\n",
      "0.161174\n",
      "0.157337\n",
      "0.153591\n",
      "0.149934\n",
      "0.146364\n",
      "0.14288\n",
      "0.139478\n",
      "0.136157\n",
      "0.132916\n",
      "0.129751\n",
      "0.126662\n",
      "0.123647\n",
      "0.120703\n",
      "0.117829\n",
      "0.115024\n",
      "0.112286\n",
      "0.109612\n",
      "0.107003\n",
      "0.104455\n",
      "0.101968\n",
      "0.0995407\n",
      "0.0971708\n",
      "0.0948574\n",
      "0.092599\n",
      "0.0903944\n",
      "0.0882424\n",
      "0.0861415\n",
      "0.0840906\n",
      "0.0820887\n",
      "0.0801343\n",
      "0.0782265\n",
      "0.0763641\n",
      "0.074546\n",
      "0.0727712\n",
      "0.0710387\n",
      "0.0693474\n",
      "0.0676965\n",
      "0.0660847\n",
      "0.0645114\n",
      "0.0629755\n",
      "0.0614762\n",
      "0.0600126\n",
      "0.0585838\n",
      "0.0571891\n",
      "0.0558276\n",
      "0.0544984\n",
      "0.0532009\n",
      "0.0519344\n",
      "0.0506979\n",
      "0.0494909\n",
      "0.0483126\n",
      "0.0471624\n",
      "0.0460396\n",
      "0.0449434\n",
      "0.0438734\n",
      "0.0428289\n",
      "0.0418093\n",
      "0.0408139\n",
      "0.0398422\n",
      "0.0388936\n",
      "0.0379677\n",
      "0.0370637\n",
      "0.0361813\n",
      "0.0353199\n",
      "0.0344791\n",
      "0.0336582\n",
      "0.0328569\n",
      "0.0320746\n",
      "0.031311\n",
      "0.0305655\n",
      "0.0298378\n",
      "0.0291275\n",
      "0.028434\n",
      "0.0277571\n",
      "0.0270962\n",
      "0.0264511\n",
      "0.0258214\n",
      "0.0252066\n",
      "0.0246066\n",
      "0.0240207\n",
      "0.0234488\n",
      "0.0228906\n",
      "0.0223456\n",
      "0.0218136\n",
      "0.0212942\n",
      "0.0207873\n",
      "0.0202924\n",
      "0.0198093\n",
      "0.0193377\n",
      "0.0188773\n",
      "0.0184279\n",
      "0.0179891\n",
      "0.0175608\n",
      "0.0171428\n",
      "0.0167346\n",
      "0.0163362\n",
      "0.0159473\n",
      "0.0155676\n",
      "0.015197\n",
      "0.0148352\n",
      "0.014482\n",
      "0.0141372\n",
      "0.0138006\n",
      "0.0134721\n",
      "0.0131513\n",
      "0.0128382\n",
      "0.0125326\n",
      "0.0122342\n",
      "0.0119429\n",
      "0.0116586\n",
      "0.011381\n",
      "0.0111101\n",
      "0.0108456\n",
      "0.0105873\n",
      "0.0103353\n",
      "0.0100892\n",
      "0.00984903\n",
      "0.00961456\n",
      "0.00938564\n",
      "0.00916219\n",
      "0.00894406\n",
      "0.00873113\n",
      "0.00852325\n",
      "0.00832035\n",
      "0.00812225\n",
      "0.00792889\n",
      "0.00774011\n",
      "0.00755582\n",
      "0.00737595\n",
      "0.00720034\n",
      "0.00702891\n",
      "0.00686157\n",
      "0.00669821\n",
      "0.00653873\n",
      "0.00638307\n",
      "0.00623111\n",
      "0.00608275\n",
      "0.00593795\n",
      "0.00579657\n",
      "0.00565857\n",
      "0.00552384\n",
      "0.00539234\n",
      "0.00526396\n",
      "0.00513863\n",
      "0.0050163\n",
      "0.00489687\n",
      "0.00478029\n",
      "0.00466648\n",
      "0.00455538\n",
      "0.00444692\n",
      "0.00434107\n",
      "0.00423772\n",
      "0.00413682\n",
      "0.00403833\n",
      "0.00394219\n",
      "0.00384834\n",
      "0.00375672\n",
      "0.00366728\n",
      "0.00357996\n",
      "0.00349473\n",
      "0.00341153\n",
      "0.00333031\n",
      "0.00325102\n",
      "0.00317362\n",
      "0.00309806\n",
      "0.0030243\n",
      "0.00295231\n",
      "0.00288201\n",
      "0.00281341\n",
      "0.00274642\n",
      "0.00268103\n",
      "0.00261721\n",
      "0.00255489\n",
      "0.00249407\n",
      "0.0024347\n",
      "0.00237673\n",
      "0.00232015\n",
      "0.00226491\n",
      "0.00221098\n",
      "0.00215834\n",
      "0.00210696\n",
      "0.0020568\n",
      "0.00200783\n",
      "0.00196002\n",
      "0.00191336\n",
      "0.00186781\n",
      "0.00182333\n",
      "0.00177993\n",
      "0.00173755\n",
      "0.00169618\n",
      "0.0016558\n",
      "0.00161638\n",
      "0.00157789\n",
      "0.00154033\n",
      "0.00150365\n",
      "0.00146786\n",
      "0.0014329\n",
      "0.00139879\n",
      "0.00136549\n",
      "0.00133298\n",
      "0.00130125\n",
      "0.00127027\n",
      "0.00124003\n",
      "0.00121051\n",
      "0.00118168\n",
      "0.00115355\n",
      "0.00112609\n",
      "0.00109928\n",
      "0.0010731\n",
      "0.00104756\n",
      "0.00102262\n",
      "0.000998269\n",
      "0.000974501\n",
      "0.000951299\n",
      "0.000928652\n",
      "0.000906539\n",
      "0.000884963\n",
      "0.000863895\n",
      "0.000843326\n",
      "0.000823247\n",
      "0.000803647\n",
      "0.00078451\n",
      "0.000765839\n",
      "0.000747599\n",
      "0.000729801\n",
      "0.000712431\n",
      "0.000695468\n",
      "0.000678909\n",
      "0.000662745\n",
      "0.000646968\n",
      "0.000631563\n",
      "0.000616529\n",
      "0.00060185\n",
      "0.000587521\n",
      "0.000573531\n",
      "0.000559879\n",
      "0.000546551\n",
      "0.000533538\n",
      "0.000520834\n",
      "0.000508431\n",
      "0.000496329\n",
      "0.000484511\n",
      "0.000472978\n",
      "0.000461719\n",
      "0.000450724\n",
      "0.000439993\n",
      "0.000429518\n",
      "0.000419294\n",
      "0.000409308\n",
      "0.000399562\n",
      "0.000390052\n",
      "0.000380766\n",
      "0.000371699\n",
      "0.000362851\n",
      "0.000354208\n",
      "0.000345776\n",
      "0.000337546\n",
      "0.00032951\n",
      "0.000321661\n",
      "0.000314003\n",
      "0.000306531\n",
      "0.000299232\n",
      "0.000292105\n",
      "0.000285155\n",
      "0.000278366\n",
      "0.000271737\n",
      "0.000265268\n",
      "0.000258952\n",
      "0.000252788\n",
      "0.000246769\n",
      "0.000240894\n",
      "0.000235162\n",
      "0.000229559\n",
      "0.000224096\n",
      "0.000218759\n",
      "0.000213552\n",
      "0.000208468\n",
      "0.000203507\n",
      "0.00019866\n",
      "0.000193932\n",
      "0.000189312\n",
      "0.000184808\n",
      "0.000180406\n",
      "0.000176111\n",
      "0.000171918\n",
      "0.000167825\n",
      "0.000163831\n",
      "0.000159929\n",
      "0.000156123\n",
      "0.000152407\n",
      "0.000148776\n",
      "0.000145237\n",
      "0.000141777\n",
      "0.000138403\n",
      "0.000135108\n",
      "0.00013189\n",
      "0.000128752\n",
      "0.000125686\n",
      "0.000122693\n",
      "0.000119773\n",
      "0.000116923\n",
      "0.000114138\n",
      "0.00011142\n",
      "0.000108768\n",
      "0.000106178\n",
      "0.000103651\n",
      "0.000101181\n",
      "9.87741e-05\n",
      "9.64204e-05\n",
      "9.41257e-05\n",
      "9.18846e-05\n",
      "8.96972e-05\n",
      "8.75618e-05\n",
      "8.54774e-05\n",
      "8.3442e-05\n",
      "8.14562e-05\n",
      "7.95163e-05\n",
      "7.7623e-05\n",
      "7.57751e-05\n",
      "7.39709e-05\n",
      "7.2209e-05\n",
      "7.04914e-05\n",
      "6.88131e-05\n",
      "6.71739e-05\n",
      "6.55745e-05\n",
      "6.40132e-05\n",
      "6.24897e-05\n",
      "6.10012e-05\n",
      "5.95481e-05\n",
      "5.81303e-05\n",
      "5.67474e-05\n",
      "5.53962e-05\n",
      "5.40775e-05\n",
      "5.27899e-05\n",
      "5.15341e-05\n",
      "5.03067e-05\n",
      "4.91101e-05\n",
      "4.79392e-05\n",
      "4.67986e-05\n",
      "4.56841e-05\n",
      "4.45969e-05\n",
      "4.35337e-05\n",
      "4.24984e-05\n",
      "4.14862e-05\n",
      "4.04988e-05\n",
      "3.95349e-05\n",
      "3.85938e-05\n",
      "3.76752e-05\n",
      "3.67781e-05\n",
      "3.59025e-05\n",
      "3.50476e-05\n",
      "3.42134e-05\n",
      "3.33993e-05\n",
      "3.26032e-05\n",
      "3.18268e-05\n",
      "3.10689e-05\n",
      "3.03305e-05\n",
      "2.96076e-05\n",
      "2.89027e-05\n",
      "2.82147e-05\n",
      "2.75424e-05\n",
      "2.68872e-05\n",
      "2.6247e-05\n",
      "2.56227e-05\n",
      "2.50128e-05\n",
      "2.44171e-05\n",
      "2.38361e-05\n",
      "2.32687e-05\n",
      "2.2714e-05\n",
      "2.21731e-05\n",
      "2.16449e-05\n",
      "2.11299e-05\n",
      "2.06269e-05\n",
      "2.0136e-05\n",
      "1.96571e-05\n",
      "1.91885e-05\n",
      "1.87316e-05\n",
      "1.82856e-05\n",
      "1.78504e-05\n",
      "1.74252e-05\n",
      "1.70108e-05\n",
      "1.66054e-05\n",
      "1.62103e-05\n",
      "1.58238e-05\n",
      "1.54475e-05\n",
      "1.50801e-05\n",
      "1.47205e-05\n",
      "1.43701e-05\n",
      "1.40279e-05\n",
      "1.36935e-05\n",
      "1.33678e-05\n",
      "1.3049e-05\n",
      "1.27383e-05\n",
      "1.24356e-05\n",
      "1.21399e-05\n",
      "1.18509e-05\n",
      "1.15685e-05\n",
      "1.12931e-05\n",
      "1.10246e-05\n",
      "1.07615e-05\n",
      "1.05058e-05\n",
      "1.02556e-05\n",
      "1.00116e-05\n",
      "9.77349e-06\n",
      "9.54086e-06\n",
      "9.31349e-06\n",
      "9.09212e-06\n",
      "8.87542e-06\n",
      "8.6642e-06\n",
      "8.45784e-06\n",
      "8.25677e-06\n",
      "8.05989e-06\n",
      "7.8683e-06\n",
      "7.68036e-06\n",
      "7.49811e-06\n",
      "7.31935e-06\n",
      "7.14508e-06\n",
      "6.97476e-06\n",
      "6.8093e-06\n",
      "6.6472e-06\n",
      "6.48866e-06\n",
      "6.33448e-06\n",
      "6.18314e-06\n",
      "6.03598e-06\n",
      "5.89221e-06\n",
      "5.75186e-06\n",
      "5.61511e-06\n",
      "5.4818e-06\n",
      "5.35109e-06\n",
      "5.22345e-06\n",
      "5.09927e-06\n",
      "4.97812e-06\n",
      "4.85944e-06\n",
      "4.74371e-06\n",
      "4.63088e-06\n",
      "4.5206e-06\n",
      "4.41293e-06\n",
      "4.30792e-06\n",
      "4.20563e-06\n",
      "4.10549e-06\n",
      "4.00757e-06\n",
      "3.91221e-06\n",
      "3.81932e-06\n",
      "3.72843e-06\n",
      "3.63923e-06\n",
      "3.553e-06\n",
      "3.46824e-06\n",
      "3.38563e-06\n",
      "3.30508e-06\n",
      "3.22617e-06\n",
      "3.14947e-06\n",
      "3.07457e-06\n",
      "3.00152e-06\n",
      "2.92984e-06\n",
      "2.8599e-06\n",
      "2.79215e-06\n",
      "2.72542e-06\n",
      "2.66055e-06\n",
      "2.59715e-06\n",
      "2.53533e-06\n",
      "2.47489e-06\n",
      "2.41597e-06\n",
      "2.35844e-06\n",
      "2.30219e-06\n",
      "2.24771e-06\n",
      "2.19425e-06\n",
      "2.14202e-06\n",
      "2.091e-06\n",
      "2.04097e-06\n",
      "1.99243e-06\n",
      "1.94514e-06\n",
      "1.89881e-06\n",
      "1.85342e-06\n",
      "1.80925e-06\n",
      "1.76629e-06\n",
      "1.72422e-06\n",
      "1.683e-06\n",
      "1.6429e-06\n",
      "1.60393e-06\n",
      "1.56578e-06\n",
      "1.52856e-06\n",
      "1.49206e-06\n",
      "1.45658e-06\n",
      "1.42185e-06\n",
      "1.38788e-06\n",
      "1.35504e-06\n",
      "1.32254e-06\n",
      "1.29115e-06\n",
      "1.26044e-06\n",
      "1.23043e-06\n",
      "1.20108e-06\n",
      "1.17254e-06\n",
      "1.14467e-06\n",
      "1.11751e-06\n",
      "1.09087e-06\n",
      "1.06479e-06\n",
      "1.03939e-06\n",
      "1.01464e-06\n",
      "9.90518e-07\n",
      "9.66906e-07\n",
      "9.43964e-07\n",
      "9.21371e-07\n",
      "8.99438e-07\n",
      "8.77956e-07\n",
      "8.57163e-07\n",
      "8.36712e-07\n",
      "8.16671e-07\n",
      "7.97356e-07\n",
      "7.78479e-07\n",
      "7.5987e-07\n",
      "7.41709e-07\n",
      "7.24029e-07\n",
      "7.06974e-07\n",
      "6.90038e-07\n",
      "6.73615e-07\n",
      "6.57639e-07\n",
      "6.42004e-07\n",
      "6.26633e-07\n",
      "6.11606e-07\n",
      "5.9723e-07\n",
      "5.82927e-07\n",
      "5.69024e-07\n",
      "5.55424e-07\n",
      "5.423e-07\n",
      "5.29394e-07\n",
      "5.16782e-07\n",
      "5.0439e-07\n",
      "4.92438e-07\n",
      "4.80739e-07\n",
      "4.69387e-07\n",
      "4.58098e-07\n",
      "4.47282e-07\n",
      "4.36655e-07\n",
      "4.26351e-07\n",
      "4.16126e-07\n",
      "4.06242e-07\n",
      "3.96628e-07\n",
      "3.87096e-07\n",
      "3.77947e-07\n",
      "3.68876e-07\n",
      "3.60062e-07\n",
      "3.51595e-07\n",
      "3.43146e-07\n",
      "3.34974e-07\n",
      "3.27061e-07\n",
      "3.19196e-07\n",
      "3.11533e-07\n",
      "3.04248e-07\n",
      "2.96925e-07\n",
      "2.89899e-07\n",
      "2.83019e-07\n",
      "2.76315e-07\n",
      "2.69692e-07\n",
      "2.63258e-07\n",
      "2.57049e-07\n",
      "2.5092e-07\n",
      "2.44949e-07\n",
      "2.39056e-07\n",
      "2.33375e-07\n",
      "2.27802e-07\n",
      "2.22387e-07\n",
      "2.17096e-07\n",
      "2.11993e-07\n",
      "2.06919e-07\n",
      "2.01989e-07\n",
      "1.97182e-07\n",
      "1.92544e-07\n",
      "1.87954e-07\n",
      "1.83413e-07\n",
      "1.79116e-07\n",
      "1.74845e-07\n",
      "1.70629e-07\n",
      "1.66566e-07\n",
      "1.62636e-07\n",
      "1.58685e-07\n",
      "1.54948e-07\n",
      "1.51209e-07\n",
      "1.47601e-07\n",
      "1.4418e-07\n",
      "1.40718e-07\n",
      "1.3738e-07\n",
      "1.34151e-07\n",
      "1.30911e-07\n",
      "1.27838e-07\n",
      "1.24792e-07\n",
      "1.21769e-07\n",
      "1.18902e-07\n",
      "1.16031e-07\n",
      "1.13255e-07\n",
      "1.10586e-07\n",
      "1.07876e-07\n",
      "1.05305e-07\n",
      "1.02857e-07\n",
      "1.00367e-07\n",
      "9.79864e-08\n",
      "9.56389e-08\n",
      "9.3363e-08\n",
      "9.11498e-08\n",
      "8.89751e-08\n",
      "8.68354e-08\n",
      "8.48196e-08\n",
      "8.27567e-08\n",
      "8.08072e-08\n",
      "7.88489e-08\n",
      "7.69487e-08\n",
      "7.51492e-08\n",
      "7.33685e-08\n",
      "7.16124e-08\n",
      "6.98991e-08\n",
      "6.82669e-08\n",
      "6.66471e-08\n",
      "6.50369e-08\n",
      "6.34856e-08\n",
      "6.19493e-08\n",
      "6.05272e-08\n",
      "5.90698e-08\n",
      "5.76604e-08\n",
      "5.62617e-08\n",
      "5.49596e-08\n",
      "5.36165e-08\n",
      "5.2365e-08\n",
      "5.11459e-08\n",
      "4.99405e-08\n",
      "4.87031e-08\n",
      "4.75945e-08\n",
      "4.64132e-08\n",
      "4.53056e-08\n",
      "4.42771e-08\n",
      "4.31705e-08\n",
      "4.21679e-08\n",
      "4.11954e-08\n",
      "4.01897e-08\n",
      "3.92167e-08\n",
      "3.83214e-08\n",
      "3.73888e-08\n",
      "3.65269e-08\n",
      "3.56353e-08\n",
      "3.48191e-08\n",
      "3.39481e-08\n",
      "3.31851e-08\n",
      "3.23701e-08\n",
      "3.16203e-08\n",
      "3.08465e-08\n",
      "3.00918e-08\n",
      "2.93741e-08\n",
      "2.8712e-08\n",
      "2.7991e-08\n",
      "2.73248e-08\n",
      "2.67031e-08\n",
      "2.60671e-08\n",
      "2.54407e-08\n",
      "2.48395e-08\n",
      "2.42127e-08\n",
      "2.36581e-08\n",
      "2.30743e-08\n",
      "2.25588e-08\n",
      "2.20179e-08\n",
      "2.14738e-08\n",
      "2.09639e-08\n",
      "2.04601e-08\n",
      "1.99893e-08\n",
      "1.94807e-08\n",
      "1.90338e-08\n",
      "1.85678e-08\n",
      "1.81616e-08\n",
      "1.77141e-08\n",
      "1.72873e-08\n",
      "1.68676e-08\n",
      "1.6477e-08\n",
      "1.60818e-08\n",
      "1.57008e-08\n",
      "1.53244e-08\n",
      "1.49626e-08\n",
      "1.46107e-08\n",
      "1.42588e-08\n",
      "1.39109e-08\n",
      "1.35917e-08\n",
      "1.32635e-08\n",
      "1.29503e-08\n",
      "1.2643e-08\n",
      "1.23396e-08\n",
      "1.20467e-08\n",
      "1.17489e-08\n",
      "1.1476e-08\n",
      "1.12172e-08\n",
      "1.09393e-08\n",
      "1.06867e-08\n",
      "1.04242e-08\n",
      "1.01775e-08\n",
      "9.92513e-09\n",
      "9.69672e-09\n",
      "9.47411e-09\n",
      "9.24832e-09\n",
      "9.02527e-09\n",
      "8.81342e-09\n",
      "8.58461e-09\n",
      "8.39009e-09\n",
      "8.203e-09\n",
      "7.99923e-09\n",
      "7.81948e-09\n",
      "7.62056e-09\n",
      "7.44515e-09\n",
      "7.26898e-09\n",
      "7.08758e-09\n",
      "6.90848e-09\n",
      "6.75391e-09\n",
      "6.5869e-09\n",
      "6.43695e-09\n",
      "6.28393e-09\n",
      "6.13675e-09\n",
      "5.99227e-09\n",
      "5.8578e-09\n",
      "5.70981e-09\n",
      "5.57306e-09\n",
      "5.43243e-09\n",
      "5.31229e-09\n",
      "5.19266e-09\n",
      "5.06025e-09\n",
      "4.93503e-09\n",
      "4.81912e-09\n",
      "4.71819e-09\n",
      "4.59814e-09\n",
      "4.48725e-09\n",
      "4.38201e-09\n",
      "4.2747e-09\n",
      "4.17197e-09\n",
      "4.07846e-09\n",
      "3.97502e-09\n",
      "3.88774e-09\n",
      "3.79768e-09\n",
      "3.70195e-09\n",
      "3.61934e-09\n",
      "3.53247e-09\n",
      "3.44741e-09\n",
      "3.36071e-09\n",
      "3.28788e-09\n",
      "3.20335e-09\n",
      "3.13925e-09\n",
      "3.05724e-09\n",
      "2.9878e-09\n",
      "2.91243e-09\n",
      "2.84465e-09\n",
      "2.78412e-09\n",
      "2.7165e-09\n",
      "2.64625e-09\n",
      "2.57694e-09\n",
      "2.51805e-09\n",
      "2.45984e-09\n",
      "2.40232e-09\n",
      "2.34547e-09\n",
      "2.28931e-09\n",
      "2.23383e-09\n",
      "2.17904e-09\n",
      "2.13364e-09\n",
      "2.08873e-09\n",
      "2.03982e-09\n",
      "1.98213e-09\n",
      "1.94029e-09\n",
      "1.89343e-09\n",
      "1.85086e-09\n",
      "1.79989e-09\n",
      "1.75614e-09\n",
      "1.71044e-09\n",
      "1.67479e-09\n",
      "1.64009e-09\n",
      "1.59587e-09\n",
      "1.56199e-09\n",
      "1.51886e-09\n",
      "1.48579e-09\n",
      "1.45099e-09\n",
      "1.41371e-09\n",
      "1.37732e-09\n",
      "1.35269e-09\n",
      "1.3207e-09\n",
      "1.28553e-09\n",
      "1.25044e-09\n",
      "1.21968e-09\n",
      "1.19655e-09\n",
      "1.16598e-09\n",
      "1.14251e-09\n",
      "1.1172e-09\n",
      "1.08765e-09\n",
      "1.06498e-09\n",
      "1.04058e-09\n",
      "1.01204e-09\n",
      "9.9017e-10\n",
      "9.66679e-10\n",
      "9.39171e-10\n",
      "9.20853e-10\n",
      "8.98172e-10\n",
      "8.73744e-10\n",
      "8.54357e-10\n",
      "8.35374e-10\n",
      "8.11713e-10\n",
      "7.92923e-10\n",
      "7.7684e-10\n",
      "7.57755e-10\n",
      "7.36151e-10\n",
      "7.20807e-10\n",
      "7.03452e-10\n",
      "6.84157e-10\n",
      "6.7185e-10\n",
      "6.59373e-10\n",
      "6.4195e-10\n",
      "6.24471e-10\n",
      "6.12733e-10\n",
      "6.00824e-10\n",
      "5.84198e-10\n",
      "5.67514e-10\n",
      "5.56344e-10\n",
      "5.45004e-10\n",
      "5.29173e-10\n",
      "5.13285e-10\n",
      "5.02684e-10\n",
      "4.89194e-10\n",
      "4.78355e-10\n",
      "4.71928e-10\n",
      "4.5917e-10\n",
      "4.48953e-10\n",
      "4.36643e-10\n",
      "4.28546e-10\n",
      "4.18154e-10\n",
      "4.06672e-10\n",
      "3.96614e-10\n",
      "3.89807e-10\n",
      "3.79714e-10\n",
      "3.69912e-10\n",
      "3.63304e-10\n",
      "3.53694e-10\n",
      "3.42641e-10\n",
      "3.33845e-10\n",
      "3.2637e-10\n",
      "3.20217e-10\n",
      "3.09075e-10\n",
      "3.02606e-10\n",
      "2.99039e-10\n",
      "2.92477e-10\n",
      "2.86008e-10\n",
      "2.75733e-10\n",
      "2.69399e-10\n",
      "2.63384e-10\n",
      "2.60044e-10\n",
      "2.53937e-10\n",
      "2.47923e-10\n",
      "2.3833e-10\n",
      "2.32451e-10\n",
      "2.26891e-10\n",
      "2.23778e-10\n",
      "2.18126e-10\n",
      "2.12566e-10\n",
      "2.06317e-10\n",
      "1.9952e-10\n",
      "1.9563e-10\n",
      "1.91665e-10\n",
      "1.87757e-10\n",
      "1.81814e-10\n",
      "1.76772e-10\n",
      "1.74143e-10\n",
      "1.70377e-10\n",
      "1.66839e-10\n",
      "1.64267e-10\n",
      "1.5908e-10\n",
      "1.55485e-10\n",
      "1.51946e-10\n",
      "1.471e-10\n",
      "1.44698e-10\n",
      "1.41274e-10\n",
      "1.38076e-10\n",
      "1.35731e-10\n",
      "1.30999e-10\n",
      "1.27745e-10\n",
      "1.24547e-10\n",
      "1.20156e-10\n",
      "1.17982e-10\n",
      "1.14898e-10\n",
      "1.12042e-10\n",
      "1.08642e-10\n",
      "1.06638e-10\n",
      "1.04663e-10\n",
      "9.96181e-11\n",
      "9.68186e-11\n",
      "9.587e-11\n",
      "9.28821e-11\n",
      "9.12515e-11\n",
      "9.04095e-11\n",
      "8.74429e-11\n",
      "8.46256e-11\n",
      "8.39009e-11\n",
      "8.11049e-11\n",
      "8.04512e-11\n",
      "7.76765e-11\n",
      "7.68701e-11\n",
      "7.59854e-11\n",
      "7.33742e-11\n",
      "7.26921e-11\n",
      "7.0159e-11\n",
      "6.76614e-11\n",
      "6.69793e-11\n",
      "6.45031e-11\n",
      "6.3892e-11\n",
      "6.14371e-11\n",
      "6.08971e-11\n",
      "5.84635e-11\n",
      "5.77707e-11\n",
      "5.69997e-11\n",
      "[array([-0.9999969], dtype=float32), array([ 0.99999082], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "## ex2_sgd.py\n",
    "W = tf.Variable([.3], tf.float32)\n",
    "b = tf.Variable([-.3], tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "\n",
    "# create a linear model, y = w * x + b\n",
    "linear_model = W * x + b\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        print sess.run(linear_model, {x:[1,2,3,4]})\n",
    "\n",
    "\n",
    "## evaluate the model, use y' - y\n",
    "y = tf.placeholder(tf.float32)\n",
    "delta = linear_model - y\n",
    "squared_deltas = tf.square(delta)\n",
    "\n",
    "# sum all squared errors to create single scalar\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for i in range(1000):\n",
    "                sess.run(train, {x:[1,2,3,4], y:[0,-1,-2,-3]})\n",
    "                print sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]})\n",
    "        print sess.run([W, b])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output, we can check that the output of `[W, b]` is:\n",
    "    `[array([-0.9999969], dtype=float32), array([ 0.99999082], dtype=float32)]`\n",
    "    \n",
    "This mean that `W` is close to -1, `b` is close to `1`, so the linear regression model should be:\n",
    "```\n",
    "   y = -W + b\n",
    "```\n",
    "\n",
    "considering the training set with its label (x, y), it's very precise, since all are accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example is for *advanced*  topics using the TFLearn, those contain many useful libaries and can be extended to more complex layers and neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp2X7dyd\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbadba84750>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': ''}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmp2X7dyd/model.ckpt.\n",
      "INFO:tensorflow:loss = 34.9760929489, step = 1\n",
      "INFO:tensorflow:global_step/sec: 596.178\n",
      "INFO:tensorflow:loss = 0.0687134914922, step = 101\n",
      "INFO:tensorflow:global_step/sec: 722.737\n",
      "INFO:tensorflow:loss = 0.00129854834646, step = 201\n",
      "INFO:tensorflow:global_step/sec: 810.083\n",
      "INFO:tensorflow:loss = 0.000880666772232, step = 301\n",
      "INFO:tensorflow:global_step/sec: 791.408\n",
      "INFO:tensorflow:loss = 9.00838638533e-06, step = 401\n",
      "INFO:tensorflow:global_step/sec: 805.134\n",
      "INFO:tensorflow:loss = 2.117308946e-06, step = 501\n",
      "INFO:tensorflow:global_step/sec: 794.534\n",
      "INFO:tensorflow:loss = 6.34788142829e-07, step = 601\n",
      "INFO:tensorflow:global_step/sec: 793.35\n",
      "INFO:tensorflow:loss = 1.71235323197e-08, step = 701\n",
      "INFO:tensorflow:global_step/sec: 790.075\n",
      "INFO:tensorflow:loss = 4.3292887399e-09, step = 801\n",
      "INFO:tensorflow:global_step/sec: 825.288\n",
      "INFO:tensorflow:loss = 1.57076124357e-10, step = 901\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmp2X7dyd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.2591284263e-11.\n",
      "INFO:tensorflow:Starting evaluation at 2017-05-02-01:57:03\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2017-05-02-01:57:03\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 2.55333e-11\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "{'loss': 2.5533343e-11, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "# Declare list of features, we only have one real-valued feature (example2.py)\n",
    "def model(features, labels, mode):\n",
    "  # Build a linear model and predict values\n",
    "  W = tf.get_variable(\"W\", [1], dtype=tf.float64)\n",
    "  b = tf.get_variable(\"b\", [1], dtype=tf.float64)\n",
    "  y = W*features['x'] + b\n",
    "  # Loss sub-graph\n",
    "  loss = tf.reduce_sum(tf.square(y - labels))\n",
    "  # Training sub-graph\n",
    "  global_step = tf.train.get_global_step()\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "  train = tf.group(optimizer.minimize(loss),\n",
    "                   tf.assign_add(global_step, 1))\n",
    "  # ModelFnOps connects subgraphs we built to the\n",
    "  # appropriate functionality.\n",
    "  return tf.contrib.learn.ModelFnOps(\n",
    "      mode=mode, predictions=y,\n",
    "      loss=loss,\n",
    "      train_op=train)\n",
    "\n",
    "estimator = tf.contrib.learn.Estimator(model_fn=model)\n",
    "# define our data set\n",
    "x = np.array([1., 2., 3., 4.])\n",
    "y = np.array([0., -1., -2., -3.])\n",
    "input_fn = tf.contrib.learn.io.numpy_input_fn({\"x\": x}, y, 4, num_epochs=1000)\n",
    "\n",
    "# train\n",
    "estimator.fit(input_fn=input_fn, steps=1000)\n",
    "# evaluate our model\n",
    "print(estimator.evaluate(input_fn=input_fn, steps=10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the straightforward code with full implementation for every operation. In the following example, it's very interesting that the initiliazation of W and b is not important.\n",
    "\n",
    "I changed the W and b **several times**, from -100 to 100, but the result are all the same, which is reasonable, because the parameters should not be so related with the initial value.\n",
    "\n",
    "Besides, how to output the parameters? like `W` and `b`?\n",
    "\n",
    "<pre>\n",
    "```python\n",
    "print sess.run([W, b], {x:x_train, y:y_train})\n",
    "```\n",
    "</pre>\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "This is the correct way, I was thinking about too much about, pay attention to the parameters assignment \n",
    "`x:x_train, y:y_train`\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [-0.99999541], b: [ 0.99998653], loss: 1.20156e-10\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "W = tf.Variable([0.9])\n",
    "b = tf.Variable([-0.9])\n",
    "\n",
    "# Model input and output\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model =  W * x + b\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y))\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# training data\n",
    "x_train = [1, 2, 3, 4]\n",
    "y_train = [0, -1, -2, -3]\n",
    "\n",
    "# training loop\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init) # reset values\n",
    "\n",
    "for i in range(1000):\n",
    "        sess.run(train, {x:x_train, y:y_train})\n",
    "        #print sess.run([W, b, loss], {x:x_train, y:y_train})\n",
    "\n",
    "# evaluate training accuracy\n",
    "curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x:x_train, y:y_train})\n",
    "print(\"W: %s, b: %s, loss: %s\"%(curr_W, curr_b, curr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
