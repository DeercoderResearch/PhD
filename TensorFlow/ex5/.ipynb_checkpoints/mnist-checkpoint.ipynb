{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST Example(tutorial)\n",
    "=====\n",
    "\n",
    "build - inference - model, mainly used two files. This file merge them and serve as tutorials.\n",
    "\n",
    "MNIST.py: libraries, that contains the basic inference/loss/ function (cannot run directly)\n",
    "fully-connected-feed.py: function, that use the MNIST.py interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os.path\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist # this is the mnist.py file(libarary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Builds the MNIST network.\\nImplements the inference/loss/training pattern for model building.\\n1. inference() - Builds the model as far as is required for running the network\\nforward to make predictions.\\n2. loss() - Adds to the inference model the layers required to generate loss.\\n3. training() - Adds to the loss model the Ops required to generate and\\napply gradients.\\nThis file is used by the various \"fully_connected_*.py\" files and not meant to\\nbe run.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Builds the MNIST network.\n",
    "Implements the inference/loss/training pattern for model building.\n",
    "1. inference() - Builds the model as far as is required for running the network\n",
    "forward to make predictions.\n",
    "2. loss() - Adds to the inference model the layers required to generate loss.\n",
    "3. training() - Adds to the loss model the Ops required to generate and\n",
    "apply gradients.\n",
    "This file is used by the various \"fully_connected_*.py\" files and not meant to\n",
    "be run.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The MNIST dataset has 10 classes, representing the digits 0 through 9.\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# The MNIST images are always 28x28 pixels.\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inference(images, hidden1_units, hidden2_units):\n",
    "    \"\"\"Build the MNIST model up to where it may be used for inference.\n",
    "    Args:\n",
    "        images: Images placeholder, from inputs().\n",
    "        hidden1_units: Size of the first hidden layer.\n",
    "        hidden2_units: Size of the second hidden layer.\n",
    "    Returns:\n",
    "        softmax_linear: Output tensor with the computed logits.\n",
    "    \"\"\"\n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights = tf.Variable(tf.truncated_normal([IMAGE_PIXELS, hidden1_units],\n",
    "                            stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))),\n",
    "        name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden1_units]),\n",
    "                         name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\n",
    "    \n",
    "    # Hidden 2\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights = tf.Variable(tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "                            stddev=1.0 / math.sqrt(float(hidden1_units))),\n",
    "        name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden2_units]),\n",
    "                         name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "  \n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(tf.truncated_normal([hidden2_units, NUM_CLASSES],\n",
    "                            stddev=1.0 / math.sqrt(float(hidden2_units))),\n",
    "        name='weights')\n",
    "        biases = tf.Variable(tf.zeros([NUM_CLASSES]),\n",
    "                         name='biases')\n",
    "        logits = tf.matmul(hidden2, weights) + biases\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(logits, labels):\n",
    "    \"\"\"Calculates the loss from the logits and the labels.\n",
    "    Args:\n",
    "        logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "        labels: Labels tensor, int32 - [batch_size].\n",
    "    Returns:\n",
    "        loss: Loss tensor of type float.\n",
    "    \"\"\"\n",
    "    labels = tf.to_int64(labels)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='xentropy')\n",
    "    return tf.reduce_mean(cross_entropy, name='xentropy_mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(loss, learning_rate):\n",
    "    \"\"\"Sets up the training Ops.\n",
    "      Creates a summarizer to track the loss over time in TensorBoard.\n",
    "      Creates an optimizer and applies the gradients to all trainable variables.\n",
    "      The Op returned by this function is what must be passed to the\n",
    "      `sess.run()` call to cause the model to train.\n",
    "      Args:\n",
    "            loss: Loss tensor, from loss().\n",
    "            learning_rate: The learning rate to use for gradient descent.\n",
    "      Returns:\n",
    "            train_op: The Op for training.\n",
    "      \"\"\"\n",
    "    # Add a scalar summary for the snapshot loss.\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    # Create the gradient descent optimizer with the given learning rate.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    # Create a variable to track the global step.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single training step.\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    \n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluation(logits, labels):\n",
    "    \"\"\"Evaluate the quality of the logits at predicting the label.\n",
    "      Args:\n",
    "            logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "            labels: Labels tensor, int32 - [batch_size], with values in the\n",
    "            range [0, NUM_CLASSES).\n",
    "      Returns:\n",
    "            A scalar int32 tensor with the number of examples (out of batch_size)\n",
    "            that were predicted correctly.\n",
    "      \"\"\"\n",
    "    # For a classifier model, we can use the in_top_k Op.\n",
    "    # It returns a bool tensor with shape [batch_size] that is true for\n",
    "    # the examples where the label is in the top k (here k=1)\n",
    "    # of all logits for that example.\n",
    "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "    # Return the number of true entries.\n",
    "    return tf.reduce_sum(tf.cast(correct, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def placeholder_inputs(batch_size):\n",
    "    \"\"\"Generate placeholder variables to represent the input tensors.\n",
    "        These placeholders are used as inputs by the rest of the model building\n",
    "          code and will be fed from the downloaded data in the .run() loop, below.\n",
    "      Args:\n",
    "            batch_size: The batch size will be baked into both placeholders.\n",
    "      Returns:\n",
    "            images_placeholder: Images placeholder.\n",
    "            labels_placeholder: Labels placeholder.\n",
    "    \"\"\"\n",
    "    # Note that the shapes of the placeholders match the shapes of the full\n",
    "    # image and label tensors, except the first dimension is now batch_size\n",
    "    # rather than the full size of the train or test data sets.\n",
    "    images_placeholder = tf.placeholder(tf.float32, shape=(batch_size,mnist.IMAGE_PIXELS))\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "    return images_placeholder, labels_placeholder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_feed_dict(data_set, images_pl, labels_pl):\n",
    "    images_feed, labels_feed = data_set.next_batch(FLAGS.batch_size,FLAGS.fake_data)\n",
    "    feed_dict = {\n",
    "        images_pl: images_feed,\n",
    "        labels_pl: labels_feed,\n",
    "    }\n",
    "    \n",
    "    return feed_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_eval(sess,\n",
    "            eval_correct,\n",
    "            images_placeholder,\n",
    "            labels_placeholder,\n",
    "            data_set):\n",
    "    \"\"\"Runs one evaluation against the full epoch of data.\n",
    "      Args:\n",
    "        sess: The session in which the model has been trained.\n",
    "        eval_correct: The Tensor that returns the number of correct predictions.\n",
    "        images_placeholder: The images placeholder.\n",
    "        labels_placeholder: The labels placeholder.\n",
    "        data_set: The set of images and labels to evaluate, from\n",
    "        input_data.read_data_sets().\n",
    "      \"\"\"\n",
    "    # And run one epoch of eval.\n",
    "    true_count = 0  # Counts the number of correct predictions.\n",
    "    steps_per_epoch = data_set.num_examples // FLAGS.batch_size\n",
    "    num_examples = steps_per_epoch * FLAGS.batch_size\n",
    "    for step in xrange(steps_per_epoch):\n",
    "        feed_dict = fill_feed_dict(data_set,\n",
    "                               images_placeholder,\n",
    "                               labels_placeholder)\n",
    "        true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "    \n",
    "    \n",
    "    precision = float(true_count) / num_examples\n",
    "    print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' % (num_examples, true_count, precision))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\n",
    "\n",
    "    # Tell TensorFlow that the model will be built into the default Graph.\n",
    "    with tf.Graph().as_default():\n",
    "        # Generate placeholders for the images and labels.\n",
    "        images_placeholder, labels_placeholder = placeholder_inputs(FLAGS.batch_size)\n",
    "\n",
    "        # Build a Graph that computes predictions from the inference model.\n",
    "        logits = mnist.inference(images_placeholder,FLAGS.hidden1,FLAGS.hidden2)\n",
    "\n",
    "        # Add to the Graph the Ops for loss calculation.\n",
    "        loss = mnist.loss(logits, labels_placeholder)\n",
    "\n",
    "        # Add to the Graph the Ops that calculate and apply gradients.\n",
    "        train_op = mnist.training(loss, FLAGS.learning_rate)\n",
    "\n",
    "        # Add the Op to compare the logits to the labels during evaluation.\n",
    "        eval_correct = mnist.evaluation(logits, labels_placeholder)\n",
    "\n",
    "        # Build the summary Tensor based on the TF collection of Summaries.\n",
    "        summary = tf.summary.merge_all()\n",
    "\n",
    "        # Add the variable initializer Op.\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Create a saver for writing training checkpoints.\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # Create a session for running Ops on the Graph.\n",
    "        sess = tf.Session()\n",
    "\n",
    "        # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "        summary_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n",
    "\n",
    "        # And then after everything is built:\n",
    "\n",
    "        # Run the Op to initialize the variables.\n",
    "        sess.run(init)\n",
    "\n",
    "        # Start the training loop.\n",
    "        for step in xrange(FLAGS.max_steps):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Fill a feed dictionary with the actual set of images and labels\n",
    "            # for this particular training step.\n",
    "            feed_dict = fill_feed_dict(data_sets.train,images_placeholder,labels_placeholder)\n",
    "\n",
    "            # Run one step of the model.  The return values are the activations\n",
    "            # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "            # inspect the values of your Ops or variables, you may include them\n",
    "            # in the list passed to sess.run() and the value tensors will be\n",
    "            # returned in the tuple from the call.\n",
    "            _, loss_value = sess.run([train_op, loss],feed_dict=feed_dict)\n",
    "\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "            # Write the summaries and print an overview fairly often.\n",
    "            if step % 100 == 0:\n",
    "                # Print status to stdout.\n",
    "                print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "                # Update the events file.\n",
    "                summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_str, step)\n",
    "                summary_writer.flush()\n",
    "\n",
    "            # Save a checkpoint and evaluate the model periodically.\n",
    "            if (step + 1) % 1000 == 0 or (step + 1) == FLAGS.max_steps:\n",
    "                checkpoint_file = os.path.join(FLAGS.log_dir, 'model.ckpt')\n",
    "                saver.save(sess, checkpoint_file, global_step=step)\n",
    "                # Evaluate against the training set.\n",
    "                print('Training Data Eval:')\n",
    "                do_eval(sess,\n",
    "                    eval_correct,\n",
    "                    images_placeholder,\n",
    "                    labels_placeholder,\n",
    "                    data_sets.train)\n",
    "                # Evaluate against the validation set.\n",
    "                print('Validation Data Eval:')\n",
    "                do_eval(sess,\n",
    "                    eval_correct,\n",
    "                    images_placeholder,\n",
    "                    labels_placeholder,\n",
    "                    data_sets.validation)\n",
    "                # Evaluate against the test set.\n",
    "                print('Test Data Eval:')\n",
    "                do_eval(sess,\n",
    "                    eval_correct,\n",
    "                    images_placeholder,\n",
    "                    labels_placeholder,\n",
    "                    data_sets.test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Entry\n",
    "\n",
    "The following code is the entry of the function, that can start from the command line, or notebook.\n",
    "\n",
    "* if there is no parameters passed, then they will use the default parameters\n",
    "* otherwise, they will use the passed parameters.\n",
    "\n",
    "for example, here I use the command to only use 1000 iterations(max-steps):\n",
    "\n",
    "        [cliu@ycao-hadoop2 mnist]$ python fully_connected_feed.py --max_steps=1000\n",
    "\n",
    "The entry is the `if __name=='__main__'` part, that accpets the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=100, fake_data=False, hidden1=128, hidden2=32, input_data_dir='/tmp/tensorflow/mnist/input_data', learning_rate=0.01, log_dir='/tmp/tensorflow/mnist/logs/fully_connected_feed', max_steps=20000)\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n",
      "Step 0: loss = 2.32 (0.014 sec)\n",
      "Step 100: loss = 2.13 (0.002 sec)\n",
      "Step 200: loss = 1.86 (0.002 sec)\n",
      "Step 300: loss = 1.57 (0.002 sec)\n",
      "Step 400: loss = 1.24 (0.002 sec)\n",
      "Step 500: loss = 0.90 (0.002 sec)\n",
      "Step 600: loss = 0.70 (0.002 sec)\n",
      "Step 700: loss = 0.72 (0.002 sec)\n",
      "Step 800: loss = 0.60 (0.002 sec)\n",
      "Step 900: loss = 0.47 (0.002 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 47099  Precision @ 1: 0.8563\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4308  Precision @ 1: 0.8616\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 8628  Precision @ 1: 0.8628\n",
      "Step 1000: loss = 0.42 (0.003 sec)\n",
      "Step 1100: loss = 0.56 (0.090 sec)\n",
      "Step 1200: loss = 0.50 (0.002 sec)\n",
      "Step 1300: loss = 0.52 (0.002 sec)\n",
      "Step 1400: loss = 0.37 (0.002 sec)\n",
      "Step 1500: loss = 0.46 (0.002 sec)\n",
      "Step 1600: loss = 0.46 (0.002 sec)\n",
      "Step 1700: loss = 0.41 (0.002 sec)\n",
      "Step 1800: loss = 0.39 (0.002 sec)\n",
      "Step 1900: loss = 0.58 (0.002 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 49219  Precision @ 1: 0.8949\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4508  Precision @ 1: 0.9016\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9002  Precision @ 1: 0.9002\n",
      "Step 2000: loss = 0.30 (0.003 sec)\n",
      "Step 2100: loss = 0.33 (0.002 sec)\n",
      "Step 2200: loss = 0.42 (0.087 sec)\n",
      "Step 2300: loss = 0.26 (0.002 sec)\n",
      "Step 2400: loss = 0.36 (0.002 sec)\n",
      "Step 2500: loss = 0.33 (0.002 sec)\n",
      "Step 2600: loss = 0.34 (0.002 sec)\n",
      "Step 2700: loss = 0.32 (0.002 sec)\n",
      "Step 2800: loss = 0.34 (0.002 sec)\n",
      "Step 2900: loss = 0.28 (0.002 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 49898  Precision @ 1: 0.9072\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4580  Precision @ 1: 0.9160\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9140  Precision @ 1: 0.9140\n",
      "Step 3000: loss = 0.33 (0.003 sec)\n",
      "Step 3100: loss = 0.29 (0.002 sec)\n",
      "Step 3200: loss = 0.32 (0.002 sec)\n",
      "Step 3300: loss = 0.38 (0.087 sec)\n",
      "Step 3400: loss = 0.26 (0.002 sec)\n",
      "Step 3500: loss = 0.16 (0.002 sec)\n",
      "Step 3600: loss = 0.29 (0.002 sec)\n",
      "Step 3700: loss = 0.21 (0.002 sec)\n",
      "Step 3800: loss = 0.39 (0.002 sec)\n",
      "Step 3900: loss = 0.44 (0.002 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 50508  Precision @ 1: 0.9183\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4615  Precision @ 1: 0.9230\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9207  Precision @ 1: 0.9207\n",
      "Step 4000: loss = 0.33 (0.003 sec)\n",
      "Step 4100: loss = 0.31 (0.002 sec)\n",
      "Step 4200: loss = 0.18 (0.002 sec)\n",
      "Step 4300: loss = 0.21 (0.002 sec)\n",
      "Step 4400: loss = 0.19 (0.087 sec)\n",
      "Step 4500: loss = 0.23 (0.002 sec)\n",
      "Step 4600: loss = 0.38 (0.002 sec)\n",
      "Step 4700: loss = 0.30 (0.002 sec)\n",
      "Step 4800: loss = 0.33 (0.002 sec)\n",
      "Step 4900: loss = 0.14 (0.002 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 50837  Precision @ 1: 0.9243\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4653  Precision @ 1: 0.9306\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9250  Precision @ 1: 0.9250\n",
      "Step 5000: loss = 0.35 (0.003 sec)\n",
      "Step 5100: loss = 0.34 (0.002 sec)\n",
      "Step 5200: loss = 0.40 (0.002 sec)\n",
      "Step 5300: loss = 0.20 (0.002 sec)\n",
      "Step 5400: loss = 0.12 (0.002 sec)\n",
      "Step 5500: loss = 0.28 (0.091 sec)\n",
      "Step 5600: loss = 0.22 (0.002 sec)\n",
      "Step 5700: loss = 0.22 (0.002 sec)\n",
      "Step 5800: loss = 0.21 (0.002 sec)\n",
      "Step 5900: loss = 0.31 (0.002 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 51149  Precision @ 1: 0.9300\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4677  Precision @ 1: 0.9354\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9330  Precision @ 1: 0.9330\n",
      "Step 6000: loss = 0.18 (0.003 sec)\n",
      "Step 6100: loss = 0.24 (0.002 sec)\n",
      "Step 6200: loss = 0.28 (0.002 sec)\n",
      "Step 6300: loss = 0.24 (0.002 sec)\n",
      "Step 6400: loss = 0.20 (0.002 sec)\n",
      "Step 6500: loss = 0.17 (0.002 sec)\n",
      "Step 6600: loss = 0.35 (0.086 sec)\n",
      "Step 6700: loss = 0.30 (0.002 sec)\n",
      "Step 6800: loss = 0.27 (0.002 sec)\n",
      "Step 6900: loss = 0.15 (0.002 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 51438  Precision @ 1: 0.9352\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4700  Precision @ 1: 0.9400\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9356  Precision @ 1: 0.9356\n",
      "Step 7000: loss = 0.18 (0.003 sec)\n",
      "Step 7100: loss = 0.18 (0.002 sec)\n",
      "Step 7200: loss = 0.19 (0.002 sec)\n",
      "Step 7300: loss = 0.34 (0.002 sec)\n",
      "Step 7400: loss = 0.20 (0.002 sec)\n",
      "Step 7500: loss = 0.13 (0.002 sec)\n",
      "Step 7600: loss = 0.18 (0.002 sec)\n",
      "Step 7700: loss = 0.25 (0.087 sec)\n",
      "Step 7800: loss = 0.07 (0.002 sec)\n",
      "Step 7900: loss = 0.22 (0.002 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 51604  Precision @ 1: 0.9383\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4723  Precision @ 1: 0.9446\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9378  Precision @ 1: 0.9378\n",
      "Step 8000: loss = 0.13 (0.003 sec)\n",
      "Step 8100: loss = 0.23 (0.002 sec)\n",
      "Step 8200: loss = 0.36 (0.002 sec)\n",
      "Step 8300: loss = 0.23 (0.002 sec)\n",
      "Step 8400: loss = 0.14 (0.002 sec)\n",
      "Step 8500: loss = 0.13 (0.002 sec)\n",
      "Step 8600: loss = 0.21 (0.002 sec)\n",
      "Step 8700: loss = 0.35 (0.002 sec)\n",
      "Step 8800: loss = 0.34 (0.087 sec)\n",
      "Step 8900: loss = 0.12 (0.002 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 51869  Precision @ 1: 0.9431\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4741  Precision @ 1: 0.9482\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9409  Precision @ 1: 0.9409\n",
      "Step 9000: loss = 0.16 (0.003 sec)\n",
      "Step 9100: loss = 0.12 (0.002 sec)\n",
      "Step 9200: loss = 0.16 (0.002 sec)\n",
      "Step 9300: loss = 0.28 (0.002 sec)\n",
      "Step 9400: loss = 0.25 (0.002 sec)\n",
      "Step 9500: loss = 0.15 (0.002 sec)\n",
      "Step 9600: loss = 0.17 (0.002 sec)\n",
      "Step 9700: loss = 0.28 (0.002 sec)\n",
      "Step 9800: loss = 0.16 (0.002 sec)\n",
      "Step 9900: loss = 0.28 (0.091 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 52013  Precision @ 1: 0.9457\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4766  Precision @ 1: 0.9532\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9447  Precision @ 1: 0.9447\n",
      "Step 10000: loss = 0.14 (0.003 sec)\n",
      "Step 10100: loss = 0.20 (0.002 sec)\n",
      "Step 10200: loss = 0.16 (0.002 sec)\n",
      "Step 10300: loss = 0.19 (0.002 sec)\n",
      "Step 10400: loss = 0.08 (0.002 sec)\n",
      "Step 10500: loss = 0.17 (0.002 sec)\n",
      "Step 10600: loss = 0.11 (0.002 sec)\n",
      "Step 10700: loss = 0.10 (0.002 sec)\n",
      "Step 10800: loss = 0.22 (0.002 sec)\n",
      "Step 10900: loss = 0.26 (0.002 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 52260  Precision @ 1: 0.9502\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4771  Precision @ 1: 0.9542\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9471  Precision @ 1: 0.9471\n",
      "Step 11000: loss = 0.17 (0.089 sec)\n",
      "Step 11100: loss = 0.12 (0.002 sec)\n",
      "Step 11200: loss = 0.14 (0.002 sec)\n",
      "Step 11300: loss = 0.25 (0.002 sec)\n",
      "Step 11400: loss = 0.15 (0.002 sec)\n",
      "Step 11500: loss = 0.20 (0.002 sec)\n",
      "Step 11600: loss = 0.21 (0.002 sec)\n",
      "Step 11700: loss = 0.09 (0.002 sec)\n",
      "Step 11800: loss = 0.33 (0.002 sec)\n",
      "Step 11900: loss = 0.11 (0.002 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 52457  Precision @ 1: 0.9538\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4785  Precision @ 1: 0.9570\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9497  Precision @ 1: 0.9497\n",
      "Step 12000: loss = 0.16 (0.004 sec)\n",
      "Step 12100: loss = 0.16 (0.092 sec)\n",
      "Step 12200: loss = 0.11 (0.002 sec)\n",
      "Step 12300: loss = 0.06 (0.002 sec)\n",
      "Step 12400: loss = 0.10 (0.002 sec)\n",
      "Step 12500: loss = 0.15 (0.002 sec)\n",
      "Step 12600: loss = 0.27 (0.002 sec)\n",
      "Step 12700: loss = 0.21 (0.002 sec)\n",
      "Step 12800: loss = 0.15 (0.002 sec)\n",
      "Step 12900: loss = 0.17 (0.002 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 52628  Precision @ 1: 0.9569\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4789  Precision @ 1: 0.9578\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9515  Precision @ 1: 0.9515\n",
      "Step 13000: loss = 0.08 (0.003 sec)\n",
      "Step 13100: loss = 0.15 (0.002 sec)\n",
      "Step 13200: loss = 0.24 (0.090 sec)\n",
      "Step 13300: loss = 0.12 (0.002 sec)\n",
      "Step 13400: loss = 0.09 (0.002 sec)\n",
      "Step 13500: loss = 0.15 (0.002 sec)\n",
      "Step 13600: loss = 0.14 (0.002 sec)\n",
      "Step 13700: loss = 0.14 (0.002 sec)\n",
      "Step 13800: loss = 0.21 (0.002 sec)\n",
      "Step 13900: loss = 0.19 (0.002 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 52690  Precision @ 1: 0.9580\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4803  Precision @ 1: 0.9606\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9524  Precision @ 1: 0.9524\n",
      "Step 14000: loss = 0.28 (0.003 sec)\n",
      "Step 14100: loss = 0.13 (0.002 sec)\n",
      "Step 14200: loss = 0.08 (0.002 sec)\n",
      "Step 14300: loss = 0.13 (0.087 sec)\n",
      "Step 14400: loss = 0.14 (0.002 sec)\n",
      "Step 14500: loss = 0.11 (0.002 sec)\n",
      "Step 14600: loss = 0.23 (0.002 sec)\n",
      "Step 14700: loss = 0.18 (0.002 sec)\n",
      "Step 14800: loss = 0.10 (0.002 sec)\n",
      "Step 14900: loss = 0.15 (0.002 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 52753  Precision @ 1: 0.9591\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4808  Precision @ 1: 0.9616\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9549  Precision @ 1: 0.9549\n",
      "Step 15000: loss = 0.13 (0.003 sec)\n",
      "Step 15100: loss = 0.16 (0.002 sec)\n",
      "Step 15200: loss = 0.24 (0.002 sec)\n",
      "Step 15300: loss = 0.19 (0.002 sec)\n",
      "Step 15400: loss = 0.10 (0.088 sec)\n",
      "Step 15500: loss = 0.05 (0.002 sec)\n",
      "Step 15600: loss = 0.03 (0.002 sec)\n",
      "Step 15700: loss = 0.15 (0.002 sec)\n",
      "Step 15800: loss = 0.20 (0.002 sec)\n",
      "Step 15900: loss = 0.19 (0.002 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 52901  Precision @ 1: 0.9618\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4814  Precision @ 1: 0.9628\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9571  Precision @ 1: 0.9571\n",
      "Step 16000: loss = 0.09 (0.004 sec)\n",
      "Step 16100: loss = 0.21 (0.002 sec)\n",
      "Step 16200: loss = 0.08 (0.002 sec)\n",
      "Step 16300: loss = 0.07 (0.002 sec)\n",
      "Step 16400: loss = 0.21 (0.002 sec)\n",
      "Step 16500: loss = 0.10 (0.087 sec)\n",
      "Step 16600: loss = 0.22 (0.002 sec)\n",
      "Step 16700: loss = 0.11 (0.002 sec)\n",
      "Step 16800: loss = 0.14 (0.002 sec)\n",
      "Step 16900: loss = 0.17 (0.002 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 53003  Precision @ 1: 0.9637\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4821  Precision @ 1: 0.9642\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9591  Precision @ 1: 0.9591\n",
      "Step 17000: loss = 0.09 (0.003 sec)\n",
      "Step 17100: loss = 0.07 (0.002 sec)\n",
      "Step 17200: loss = 0.15 (0.002 sec)\n",
      "Step 17300: loss = 0.09 (0.002 sec)\n",
      "Step 17400: loss = 0.16 (0.002 sec)\n",
      "Step 17500: loss = 0.31 (0.002 sec)\n",
      "Step 17600: loss = 0.06 (0.090 sec)\n",
      "Step 17700: loss = 0.06 (0.002 sec)\n",
      "Step 17800: loss = 0.05 (0.002 sec)\n",
      "Step 17900: loss = 0.09 (0.002 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 53082  Precision @ 1: 0.9651\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4829  Precision @ 1: 0.9658\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9611  Precision @ 1: 0.9611\n",
      "Step 18000: loss = 0.08 (0.003 sec)\n",
      "Step 18100: loss = 0.19 (0.002 sec)\n",
      "Step 18200: loss = 0.15 (0.002 sec)\n",
      "Step 18300: loss = 0.08 (0.002 sec)\n",
      "Step 18400: loss = 0.21 (0.002 sec)\n",
      "Step 18500: loss = 0.10 (0.002 sec)\n",
      "Step 18600: loss = 0.18 (0.002 sec)\n",
      "Step 18700: loss = 0.20 (0.087 sec)\n",
      "Step 18800: loss = 0.06 (0.002 sec)\n",
      "Step 18900: loss = 0.12 (0.002 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 53142  Precision @ 1: 0.9662\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4830  Precision @ 1: 0.9660\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9628  Precision @ 1: 0.9628\n",
      "Step 19000: loss = 0.06 (0.003 sec)\n",
      "Step 19100: loss = 0.09 (0.002 sec)\n",
      "Step 19200: loss = 0.07 (0.002 sec)\n",
      "Step 19300: loss = 0.16 (0.002 sec)\n",
      "Step 19400: loss = 0.11 (0.002 sec)\n",
      "Step 19500: loss = 0.06 (0.002 sec)\n",
      "Step 19600: loss = 0.07 (0.002 sec)\n",
      "Step 19700: loss = 0.25 (0.002 sec)\n",
      "Step 19800: loss = 0.07 (0.132 sec)\n",
      "Step 19900: loss = 0.05 (0.002 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 53195  Precision @ 1: 0.9672\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4832  Precision @ 1: 0.9664\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9637  Precision @ 1: 0.9637\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def main(_):\n",
    "    # the following parameters are constructed: (by the if __name == '__main__', use default parameter)\n",
    "    # Namespace(batch_size=100, fake_data=False, hidden1=128, hidden2=32, input_data_dir='/tmp/tensorflow/mnist/input_data',\n",
    "    # learning_rate=0.01, log_dir='/tmp/tensorflow/mnist/logs/fully_connected_feed', max_steps=20000)\n",
    "    #print(FLAGS)\n",
    "    \n",
    "    if tf.gfile.Exists(FLAGS.log_dir):\n",
    "        tf.gfile.DeleteRecursively(FLAGS.log_dir)\n",
    "    tf.gfile.MakeDirs(FLAGS.log_dir)\n",
    "    run_training()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "      '--learning_rate',\n",
    "      type=float,\n",
    "      default=0.01,\n",
    "      help='Initial learning rate.')\n",
    "\n",
    "    parser.add_argument(\n",
    "      '--max_steps',\n",
    "      type=int,\n",
    "      default=20000,\n",
    "      help='Number of steps to run trainer.')\n",
    "    \n",
    "    parser.add_argument(\n",
    "      '--hidden1',\n",
    "      type=int,\n",
    "      default=128,\n",
    "      help='Number of units in hidden layer 1.')\n",
    "    \n",
    "    parser.add_argument(\n",
    "      '--hidden2',\n",
    "      type=int,\n",
    "      default=32,\n",
    "      help='Number of units in hidden layer 2.')\n",
    "    \n",
    "    parser.add_argument(\n",
    "      '--batch_size',\n",
    "      type=int,\n",
    "      default=100,\n",
    "      help='Batch size.  Must divide evenly into the dataset sizes.')\n",
    "    \n",
    "    parser.add_argument(\n",
    "      '--input_data_dir',\n",
    "      type=str,\n",
    "      default='/tmp/tensorflow/mnist/input_data',\n",
    "      help='Directory to put the input data.')\n",
    "    \n",
    "    parser.add_argument(\n",
    "      '--log_dir',\n",
    "      type=str,\n",
    "      default='/tmp/tensorflow/mnist/logs/fully_connected_feed',\n",
    "      help='Directory to put the log data.')\n",
    "    \n",
    "    parser.add_argument(\n",
    "      '--fake_data',\n",
    "      default=False,\n",
    "      help='If true, uses fake data for unit testing.',\n",
    "      action='store_true')\n",
    "\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
